\
        // src/main.cpp
        #include <iostream>
        #include <vector>
        #include <string>
        #include <memory>
        #include <fstream>
        #include <sstream>
        #include <onnxruntime_cxx_api.h>

        const char* copyString(std::string s)
        {
            const char* s2;

            // std::string::c_str() method
            s2 = s.c_str();

            return s2;
        }
        
        // Simple CSV loader: expects each row to contain 4 float features.
        std::vector<std::vector<float>> load_csv(const std::string& path) {
            std::vector<std::vector<float>> samples;
            std::ifstream ifs(path);
            if (!ifs) {
                throw std::runtime_error("Cannot open CSV file: " + path);
            }
            std::string line;
            while (std::getline(ifs, line)) {
                if(line.empty()) continue;
                std::stringstream ss(line);
                std::vector<float> row;
                std::string cell;
                while (std::getline(ss, cell, ',')) {
                    if(cell.size()==0) continue;
                    row.push_back(std::stof(cell));
                }
                if (!row.empty()) {
                    if (row.size() != 4) {
                        throw std::runtime_error("Each row must have 4 features. Found: " + std::to_string(row.size()));
                    }
                    samples.push_back(row);
                }
            }
            return samples;
        }

        int main(int argc, char** argv) {
            std::string model_path = "/workspace/models/iris_logreg.onnx";
            std::string csv_path;
            if (argc > 1) csv_path = argv[1];

            // default sample if no CSV provided
            std::vector<std::vector<float>> samples;
            if (!csv_path.empty()) {
                try {
                    samples = load_csv(csv_path);
                } catch (const std::exception &ex) {
                    std::cerr << "Error loading CSV: " << ex.what() << std::endl;
                    return 2;
                }
            } else {
                samples = {{5.1f,3.5f,1.4f,0.2f}};
                std::cout << "No CSV provided. Using default sample." << std::endl;
            }

            std::string instance_name{ "Mfg Process Metadata Predictor inference" };

            try {
                Ort::Env env(ORT_LOGGING_LEVEL_WARNING, instance_name.c_str());
                Ort::SessionOptions session_options;
                session_options.SetIntraOpNumThreads(1);

                Ort::Session session(env, model_path.c_str(), session_options);

                Ort::AllocatorWithDefaultOptions allocator;

                // Input metadata
                std::string input_name_std = session.GetInputNameAllocated(0, allocator.operator OrtAllocator * ()).get();
                const char* input_name = copyString(input_name_std);
                //char* input_name = session.GetInputName(0, allocator);
                //std::string input_name = session.GetInputNameAllocated(0, allocator).get();
                std::cout << "Input name: " << input_name << std::endl;
                auto input_type_info = session.GetInputTypeInfo(0);
                auto input_tensor_info = input_type_info.GetTensorTypeAndShapeInfo();
                std::vector<int64_t> input_node_dims = input_tensor_info.GetShape();

                // Prepare batch tensor
                size_t batch = samples.size();
                size_t features = samples[0].size();
                std::vector<int64_t> input_shape = { static_cast<int64_t>(batch), static_cast<int64_t>(features) };
                std::vector<float> input_tensor_values;
                input_tensor_values.reserve(batch * features);
                for (const auto &r : samples) {
                    for (float v : r) input_tensor_values.push_back(v);
                }

                Ort::MemoryInfo memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
                Ort::Value input_tensor = Ort::Value::CreateTensor<float>(memory_info, input_tensor_values.data(), input_tensor_values.size(), input_shape.data(), input_shape.size());

                // Output names
                size_t num_output_nodes = session.GetOutputCount();
                std::cout << "Number of outputs: " << num_output_nodes << std::endl;
                std::vector<const char*> output_names;
                //std::vector<std::string> output_names;
                for (size_t i = 0; i < num_output_nodes; ++i) {
                    //std::string out_name_std = session.GetOutputNameAllocated(i, allocator).get();
                    //const char* out_name = copyString(out_name_std);
                    //char* out_name = session.GetOutputName(i, allocator);
                    //char *out_name = session.GetOutputNameAllocated(i, allocator).get();
                    std::string out_name = session.GetOutputNameAllocated(i, allocator).get();
                    //std::cout << "Output name: " << out_name << std::endl;
                    std::cout << "Output " << i << " name: " << out_name << "\n";
                    output_names.push_back(out_name.c_str());
                }
                // std::string output_names_str[2] = {"Label", "Probabilities"};
                // for (int i=0; i < 1; i++)
                // {
                //     std::cout << "Output " << i << " name: " << output_names_str[i] << "\n";
                //     output_names.push_back(output_names_str[i].c_str());
                // }
                
                std::cout << "Ouptput names size: " << output_names.size() << "\n";

                // Run
                auto output_tensors = session.Run(Ort::RunOptions{nullptr},
                                                  &input_name, &input_tensor, 1,
                                                  output_names.data(), output_names.size());
                // auto output_tensors = session.Run(Ort::RunOptions{nullptr},
                //                                   &input_name, &input_tensor, 1);

                // Print results for each output
                for (size_t i = 0; i < output_tensors.size(); ++i) {
                    auto &val = output_tensors[i];
                    auto type_info = val.GetTensorTypeAndShapeInfo();
                    std::vector<int64_t> shape = type_info.GetShape();
                    size_t total_len = type_info.GetElementCount();
                    std::cout << "Output " << i << " shape:";
                    for (auto d : shape) std::cout << " " << d;
                    std::cout << "\n";

                    if (type_info.GetElementType() == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) {
                        float* out_data = val.GetTensorMutableData<float>();
                        std::cout << "Output floats: ";
                        for (size_t j = 0; j < total_len; ++j) std::cout << out_data[j] << ((j+1==total_len) ? "" : ", ");
                        std::cout << std::endl;
                    } else if (type_info.GetElementType() == ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64) {
                        int64_t* out_data = val.GetTensorMutableData<int64_t>();
                        std::cout << "Output ints: ";
                        for (size_t j = 0; j < total_len; ++j) std::cout << out_data[j] << ((j+1==total_len) ? "" : ", ");
                        std::cout << std::endl;
                    } else {
                        std::cout << "Unhandled output type: " << type_info.GetElementType() << std::endl;
                    }
                }

            } catch (const Ort::Exception &ex) {
                std::cerr << "ONNX Runtime error: " << ex.what() << "\\n";
                return 2;
            } catch (const std::exception &ex) {
                std::cerr << "STD exception: " << ex.what() << "\\n";
                return 3;
            }
            return 0;
        }
